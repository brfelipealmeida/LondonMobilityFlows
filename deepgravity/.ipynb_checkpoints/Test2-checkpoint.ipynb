{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f1fbed-35c8-4184-8c95-3d7f09fe19e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50173149-71d9-4920-835c-7363e3787268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Any, Callable, Dict, IO, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "from importlib.machinery import SourceFileLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0aaaf13-24b4-4ff6-a84c-8e2978814800",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './utils.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./utils.py\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m utils \u001b[38;5;241m=\u001b[39m \u001b[43mSourceFileLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutils\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:548\u001b[0m, in \u001b[0;36m_check_name_wrapper\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1063\u001b[0m, in \u001b[0;36mload_module\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:888\u001b[0m, in \u001b[0;36mload_module\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:290\u001b[0m, in \u001b[0;36m_load_module_shim\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:719\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1016\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1073\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './utils.py'"
     ]
    }
   ],
   "source": [
    "path = './utils.py'\n",
    "utils = SourceFileLoader('utils', path).load_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84541426-946a-4e6d-97b3-141489b5b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    ids = [item[2] for item in batch]\n",
    "    #target = torch.LongTensor(target)\n",
    "    return [data, target], ids\n",
    "\n",
    "class FlowDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 list_IDs: List[str],\n",
    "                 tileid2oa2features2vals: Dict,\n",
    "                 o2d2flow: Dict,\n",
    "                 oa2features: Dict,\n",
    "                 oa2pop: Dict,\n",
    "                 oa2centroid: Dict,\n",
    "                 dim_dests: int,\n",
    "                 frac_true_dest: float, \n",
    "                 model: str\n",
    "                ) -> None:\n",
    "        'Initialization'\n",
    "        self.list_IDs = list_IDs\n",
    "        self.tileid2oa2features2vals = tileid2oa2features2vals\n",
    "        self.o2d2flow = o2d2flow\n",
    "        self.oa2features = oa2features\n",
    "        self.oa2pop = oa2pop\n",
    "        self.oa2centroid = oa2centroid\n",
    "        self.dim_dests = dim_dests\n",
    "        self.frac_true_dest = frac_true_dest\n",
    "        self.model = model\n",
    "        self.oa2tile = {oa:tile for tile,oa2v in tileid2oa2features2vals.items() for oa in oa2v.keys()}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def get_features(self, oa_origin, oa_destination):\n",
    "        oa2features = self.oa2features\n",
    "        oa2centroid = self.oa2centroid\n",
    "        dist_od = utils.earth_distance(oa2centroid[oa_origin], oa2centroid[oa_destination])\n",
    "\n",
    "        return oa2features[oa_origin] + oa2features[oa_destination] + [dist_od]\n",
    "\n",
    "    def get_flow(self, oa_origin, oa_destination):\n",
    "        o2d2flow = self.o2d2flow\n",
    "        try:\n",
    "            return o2d2flow[oa_origin][oa_destination]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    def get_destinations(self, oa, size_train_dest, all_locs_in_train_region):\n",
    "        o2d2flow = self.o2d2flow\n",
    "        frac_true_dest = self.frac_true_dest\n",
    "        try:\n",
    "            true_dests_all = list(o2d2flow[oa].keys())\n",
    "        except KeyError:\n",
    "            true_dests_all = []\n",
    "        size_true_dests = min(int(size_train_dest * frac_true_dest), len(true_dests_all))\n",
    "        size_fake_dests = size_train_dest - size_true_dests\n",
    "\n",
    "        true_dests = np.random.choice(true_dests_all, size=size_true_dests, replace=False)\n",
    "        fake_dests_all = list(set(all_locs_in_train_region) - set(true_dests))\n",
    "        fake_dests = np.random.choice(fake_dests_all, size=size_fake_dests, replace=False)\n",
    "\n",
    "        dests = np.concatenate((true_dests, fake_dests))\n",
    "        np.random.shuffle(dests)\n",
    "        return dests\n",
    "\n",
    "    def get_X_T(self, origin_locs, dest_locs):\n",
    "\n",
    "        X, T = [], []\n",
    "        for en, i in enumerate(origin_locs):\n",
    "            X += [[]]\n",
    "            T += [[]]\n",
    "            for j in dest_locs[en]:\n",
    "                X[-1] += [self.get_features(i, j)]\n",
    "                T[-1] += [self.get_flow(i, j)]\n",
    "\n",
    "        teX = torch.from_numpy(np.array(X)).float()\n",
    "        teT = torch.from_numpy(np.array(T)).float()\n",
    "        return teX, teT\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "\n",
    "        tileid2oa2features2vals = self.tileid2oa2features2vals\n",
    "        dim_dests = self.dim_dests\n",
    "        oa2tile = self.oa2tile\n",
    "\n",
    "        # Select sample (tile)\n",
    "        sampled_origins = [self.list_IDs[index]]\n",
    "        tile_ID = oa2tile[sampled_origins[0]]\n",
    "\n",
    "        all_locs_in_train_region = list(tileid2oa2features2vals[tile_ID].keys())\n",
    "        size_train_dest = min(dim_dests, len(all_locs_in_train_region))\n",
    "        sampled_dests = [self.get_destinations(oa, size_train_dest, all_locs_in_train_region)\n",
    "                         for oa in sampled_origins]\n",
    "\n",
    "        sampled_trX, sampled_trT = self.get_X_T(sampled_origins, sampled_dests)\n",
    "\n",
    "\n",
    "        return sampled_trX, sampled_trT, sampled_origins\n",
    "\n",
    "    def __getitem_tile__(self, index: int) -> Tuple[Any, Any]:\n",
    "        'Generates one sample of data (one tile)'\n",
    "\n",
    "        tileid2oa2features2vals = self.tileid2oa2features2vals\n",
    "        dim_dests = self.dim_dests\n",
    "        tile_ID = self.list_IDs[index]\n",
    "        sampled_origins = list(tileid2oa2features2vals[tile_ID].keys())\n",
    "\n",
    "        # Select a subset of OD pairs\n",
    "        train_locs = sampled_origins\n",
    "        all_locs_in_train_region = train_locs\n",
    "        size_train_dest = min(dim_dests, len(all_locs_in_train_region))\n",
    "        sampled_dests = [self.get_destinations(oa, size_train_dest, all_locs_in_train_region)\n",
    "                         for oa in sampled_origins]\n",
    "\n",
    "        # get the features and flows\n",
    "        sampled_trX, sampled_trT = self.get_X_T(sampled_origins, sampled_dests)\n",
    "\n",
    "        return sampled_trX, sampled_trT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee1c28-9687-4103-8af8-319ff56bebd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506004e-4b09-4138-b3cb-f97999c6f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from importlib.machinery import SourceFileLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b1e6a-4c2e-4093-a2e6-d47a727ea350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='DeepGravity')\n",
    "parser.add_argument('--batch_size', type=int, default=1, metavar='N',\n",
    "                    help='input batch size for training (default: 1)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1, metavar='N',\n",
    "                    help='input batch size for testing (default: 1)')\n",
    "parser.add_argument('--epochs', type=int, default=15, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=5e-6, metavar='LR',\n",
    "                    help='learning rate (default: 5e-6)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "parser.add_argument('--seed', type=int, default=1234, metavar='S',\n",
    "                    help='random seed (default: 1234)')\n",
    "parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--device', default='cpu',\n",
    "                    help='Wheter this is running on cpu or gpu')\n",
    "parser.add_argument('--mode', default='train', help='Can be train or test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c73565-4f87-40fd-8cb4-bcd9ce5c4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model arguments\n",
    "parser.add_argument('--tessellation-area', default='United Kingdom',\n",
    "                    help='The area to tessel if a tessellation is not provided')\n",
    "parser.add_argument('--tessellation-size', type=int, default=25000,\n",
    "                    help='The tessellation size (meters) if a tessellation is not provided')\n",
    "parser.add_argument('--dataset', default='new_york', help='The dataset to use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e08dc-3a9c-40d8-9c02-502804f8edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset arguments \n",
    "parser.add_argument('--tile-id-column', default='tile_ID', help='Column name of tile\\'s identifier')\n",
    "parser.add_argument('--tile-geometry', default='geometry', help='Column name of tile\\'s geometry')\n",
    "\n",
    "parser.add_argument('--oa-id-column', default='oa_ID', help='Column name of oa\\'s identifier')\n",
    "parser.add_argument('--oa-geometry', default='geometry', help='Column name of oa\\'s geometry')\n",
    "\n",
    "parser.add_argument('--flow-origin-column', default='origin', help='Column name of flows\\' origin')\n",
    "parser.add_argument('--flow-destination-column', default='destination', help='Column name of flows\\' destination')\n",
    "parser.add_argument('--flow-flows-column', default='flow', help='Column name of flows\\' actual value')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d83ff5-8faf-4a4a-8b1f-a4ee3c15feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings\n",
    "model_type = 'DG'\n",
    "data_name = args.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce28a7f-5bbb-49e5-b059-bda6bf1798f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seeds\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6ca42-70b1-4695-8c4f-1f8584413e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading DataLoader and utilities\n",
    "path = './data_loader.py'\n",
    "dgd = SourceFileLoader('dg_data', path).load_module()\n",
    "path = './utils.py'\n",
    "utils = SourceFileLoader('utils', path).load_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3f545-3fb3-4e4b-88dc-3a4728f8f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device \n",
    "args.cuda = args.device.find(\"gpu\") != -1\n",
    "\n",
    "if args.device.find(\"gpu\") != -1:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    torch_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b745aba-e47b-4b0f-a605-8a16bd16460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if raw data exists and otherwise stop the execution\n",
    "if not os.path.isdir('./data/' + data_name):\n",
    "    raise ValueError('There is no dataset named ' + data_name + ' in ./data/')\n",
    "\n",
    "db_dir = './data/' + data_name\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    training_acc = 0.0\n",
    "\n",
    "    for batch_idx, data_temp in enumerate(train_loader):\n",
    "        b_data = data_temp[0]\n",
    "        b_target = data_temp[1]\n",
    "        ids = data_temp[2]\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "        for data, target in zip(b_data, b_target):\n",
    "\n",
    "            if args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            output = model.forward(data)\n",
    "            loss += model.loss(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            if batch_idx * len(b_data) == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} \\tLoss: {:.6f}'.format(epoch, batch_idx * len(b_data), len(train_loader),\n",
    "                                                                     loss.item() / args.batch_size))\n",
    "\n",
    "    running_loss = running_loss / len(train_dataset)\n",
    "    training_acc = training_acc / len(train_dataset)\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.\n",
    "        test_accuracy = 0.\n",
    "        n_origins = 0\n",
    "        for batch_idx, data_temp in enumerate(test_loader):\n",
    "            b_data = data_temp[0]\n",
    "            b_target = data_temp[1]\n",
    "            ids = data_temp[2]\n",
    "            test_loss = 0.0\n",
    "\n",
    "            for data, target in zip(b_data, b_target):\n",
    "                if args.cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                output = model.forward(data)\n",
    "                test_loss += model.loss(output, target).item()\n",
    "\n",
    "                cpc = model.get_cpc(data, target)\n",
    "                test_accuracy += cpc\n",
    "                n_origins += 1\n",
    "\n",
    "            break\n",
    "\n",
    "        test_loss /= n_origins\n",
    "        test_accuracy /= n_origins\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    loc2cpc_numerator = {}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data_temp in test_loader:\n",
    "            b_data = data_temp[0]\n",
    "            b_target = data_temp[1]\n",
    "            ids = data_temp[2]\n",
    "            for id, data, target in zip(ids, b_data, b_target):\n",
    "                if args.cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                output = model.forward(data)\n",
    "                cpc = model.get_cpc(data, target, numerator_only=True)\n",
    "                loc2cpc_numerator[id[0]] = cpc\n",
    "    edf = pd.DataFrame.from_dict(loc2cpc_numerator, columns=['cpc_num'], orient='index').reset_index().rename(\n",
    "        columns={'index': 'locID'})\n",
    "    oa2tile = {oa: t for t, v in tileid2oa2features2vals.items() for oa in v.keys()}\n",
    "\n",
    "    def cpc_from_num(edf, oa2tile, o2d2flow):\n",
    "        print(edf.head())\n",
    "        edf['tile'] = edf['locID'].apply(lambda x: oa2tile[x])\n",
    "        edf['tot_flow'] = edf['locID'].apply(lambda x: sum(o2d2flow[x].values()) if x in o2d2flow else 1e-6)\n",
    "        cpc_df = pd.DataFrame(edf.groupby('tile').apply( \\\n",
    "            lambda x: x['cpc_num'].sum() / 2 / x['tot_flow'].sum()), \\\n",
    "            columns=['cpc']).reset_index()\n",
    "        return cpc_df\n",
    "\n",
    "    cpc_df = cpc_from_num(edf, oa2tile, o2d2flow)\n",
    "    print('Average CPC of test tiles: {cpc_df.cpc.mean():.4f}  stdev: {cpc_df.cpc.std():.4f}')\n",
    "\n",
    "    fname = './results/tile2cpc_{}_{}.csv'.format(model_type, args.dataset)\n",
    "\n",
    "    cpc_df.to_csv(fname, index=False)\n",
    "\n",
    "\n",
    "utils.tessellation_definition(db_dir, args.tessellation_area, args.tessellation_size)\n",
    "\n",
    "tileid2oa2features2vals, oa_gdf, flow_df, oa2pop, oa2features, od2flow, oa2centroid = utils.load_data(db_dir,\n",
    "                                                                                                      args.tile_id_column,\n",
    "                                                                                                      args.tile_geometry,\n",
    "                                                                                                      args.oa_id_column,\n",
    "                                                                                                      args.oa_geometry,\n",
    "                                                                                                      args.flow_origin_column,\n",
    "                                                                                                      args.flow_destination_column,\n",
    "                                                                                                      args.flow_flows_column)\n",
    "\n",
    "oa2features = {oa: [np.log(oa2pop[oa])] + feats for oa, feats in oa2features.items()}\n",
    "\n",
    "o2d2flow = {}\n",
    "for (o, d), f in od2flow.items():\n",
    "    try:\n",
    "        d2f = o2d2flow[o]\n",
    "        d2f[d] = f\n",
    "    except KeyError:\n",
    "        o2d2flow[o] = {d: f}\n",
    "\n",
    "train_dataset_args = {'tileid2oa2features2vals': tileid2oa2features2vals,\n",
    "                      'o2d2flow': o2d2flow,\n",
    "                      'oa2features': oa2features,\n",
    "                      'oa2pop': oa2pop,\n",
    "                      'oa2centroid': oa2centroid,\n",
    "                      'dim_dests': 512,\n",
    "                      'frac_true_dest': 0.0,\n",
    "                      'model': model_type}\n",
    "\n",
    "test_dataset_args = {'tileid2oa2features2vals': tileid2oa2features2vals,\n",
    "                     'o2d2flow': o2d2flow,\n",
    "                     'oa2features': oa2features,\n",
    "                     'oa2pop': oa2pop,\n",
    "                     'oa2centroid': oa2centroid,\n",
    "                     'dim_dests': int(1e9),\n",
    "                     'frac_true_dest': 0.0,\n",
    "                     'model': model_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88864fee-74b7-4015-a318-18a0a2b6858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "\n",
    "train_data = [oa for t in pd.read_csv(db_dir + '/processed/train_tiles.csv', header=None, dtype=object)[0].values for oa\n",
    "              in tileid2oa2features2vals[str(t)].keys()]\n",
    "test_data = [oa for t in pd.read_csv(db_dir + '/processed/test_tiles.csv', header=None)[0].values for oa in\n",
    "             tileid2oa2features2vals[str(t)].keys()]\n",
    "\n",
    "train_dataset = dgd.FlowDataset(train_data, **train_dataset_args)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "\n",
    "test_dataset = dgd.FlowDataset(test_data, **test_dataset_args)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size)\n",
    "\n",
    "dim_input = len(train_dataset.get_features(train_data[0], train_data[0]))\n",
    "\n",
    "if args.mode == 'train':\n",
    "\n",
    "    model = utils.instantiate_model(oa2centroid, oa2features, oa2pop, dim_input, device=torch_device)\n",
    "    if args.device.find(\"gpu\") != -1:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    t0 = time.time()\n",
    "    test()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        # set new random seeds\n",
    "        torch.manual_seed(args.seed + epoch)\n",
    "        np.random.seed(args.seed + epoch)\n",
    "        random.seed(args.seed + epoch)\n",
    "\n",
    "        train(epoch)\n",
    "        test()\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: %s seconds\" % (t1 - t0))\n",
    "\n",
    "    fname = './results/model_{}_{}.pt'.format(model_type, args.dataset)\n",
    "    print('Saving model to {} ...'.format(fname))\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, fname)\n",
    "\n",
    "    print('Computing the CPC on test set, loc2cpc_numerator ...')\n",
    "\n",
    "    evaluate()\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    model = utils.instantiate_model(oa2centroid, oa2features, oa2pop, dim_input, device=torch_device)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    checkpoint = torch.load('./results/model_' + model_type + '_' + args.dataset + '.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

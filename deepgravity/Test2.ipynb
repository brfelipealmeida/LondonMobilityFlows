{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f1fbed-35c8-4184-8c95-3d7f09fe19e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50173149-71d9-4920-835c-7363e3787268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Any, Callable, Dict, IO, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "from importlib.machinery import SourceFileLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0aaaf13-24b4-4ff6-a84c-8e2978814800",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './utils.py'\n",
    "utils = SourceFileLoader('utils', path).load_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84541426-946a-4e6d-97b3-141489b5b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    ids = [item[2] for item in batch]\n",
    "    #target = torch.LongTensor(target)\n",
    "    return [data, target], ids\n",
    "\n",
    "class FlowDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 list_IDs: List[str],\n",
    "                 tileid2oa2features2vals: Dict,\n",
    "                 o2d2flow: Dict,\n",
    "                 oa2features: Dict,\n",
    "                 oa2pop: Dict,\n",
    "                 oa2centroid: Dict,\n",
    "                 dim_dests: int,\n",
    "                 frac_true_dest: float, \n",
    "                 model: str\n",
    "                ) -> None:\n",
    "        'Initialization'\n",
    "        self.list_IDs = list_IDs\n",
    "        self.tileid2oa2features2vals = tileid2oa2features2vals\n",
    "        self.o2d2flow = o2d2flow\n",
    "        self.oa2features = oa2features\n",
    "        self.oa2pop = oa2pop\n",
    "        self.oa2centroid = oa2centroid\n",
    "        self.dim_dests = dim_dests\n",
    "        self.frac_true_dest = frac_true_dest\n",
    "        self.model = model\n",
    "        self.oa2tile = {oa:tile for tile,oa2v in tileid2oa2features2vals.items() for oa in oa2v.keys()}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def get_features(self, oa_origin, oa_destination):\n",
    "        oa2features = self.oa2features\n",
    "        oa2centroid = self.oa2centroid\n",
    "        dist_od = utils.earth_distance(oa2centroid[oa_origin], oa2centroid[oa_destination])\n",
    "\n",
    "        return oa2features[oa_origin] + oa2features[oa_destination] + [dist_od]\n",
    "\n",
    "    def get_flow(self, oa_origin, oa_destination):\n",
    "        o2d2flow = self.o2d2flow\n",
    "        try:\n",
    "            return o2d2flow[oa_origin][oa_destination]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    def get_destinations(self, oa, size_train_dest, all_locs_in_train_region):\n",
    "        o2d2flow = self.o2d2flow\n",
    "        frac_true_dest = self.frac_true_dest\n",
    "        try:\n",
    "            true_dests_all = list(o2d2flow[oa].keys())\n",
    "        except KeyError:\n",
    "            true_dests_all = []\n",
    "        size_true_dests = min(int(size_train_dest * frac_true_dest), len(true_dests_all))\n",
    "        size_fake_dests = size_train_dest - size_true_dests\n",
    "\n",
    "        true_dests = np.random.choice(true_dests_all, size=size_true_dests, replace=False)\n",
    "        fake_dests_all = list(set(all_locs_in_train_region) - set(true_dests))\n",
    "        fake_dests = np.random.choice(fake_dests_all, size=size_fake_dests, replace=False)\n",
    "\n",
    "        dests = np.concatenate((true_dests, fake_dests))\n",
    "        np.random.shuffle(dests)\n",
    "        return dests\n",
    "\n",
    "    def get_X_T(self, origin_locs, dest_locs):\n",
    "\n",
    "        X, T = [], []\n",
    "        for en, i in enumerate(origin_locs):\n",
    "            X += [[]]\n",
    "            T += [[]]\n",
    "            for j in dest_locs[en]:\n",
    "                X[-1] += [self.get_features(i, j)]\n",
    "                T[-1] += [self.get_flow(i, j)]\n",
    "\n",
    "        teX = torch.from_numpy(np.array(X)).float()\n",
    "        teT = torch.from_numpy(np.array(T)).float()\n",
    "        return teX, teT\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "\n",
    "        tileid2oa2features2vals = self.tileid2oa2features2vals\n",
    "        dim_dests = self.dim_dests\n",
    "        oa2tile = self.oa2tile\n",
    "\n",
    "        # Select sample (tile)\n",
    "        sampled_origins = [self.list_IDs[index]]\n",
    "        tile_ID = oa2tile[sampled_origins[0]]\n",
    "\n",
    "        all_locs_in_train_region = list(tileid2oa2features2vals[tile_ID].keys())\n",
    "        size_train_dest = min(dim_dests, len(all_locs_in_train_region))\n",
    "        sampled_dests = [self.get_destinations(oa, size_train_dest, all_locs_in_train_region)\n",
    "                         for oa in sampled_origins]\n",
    "\n",
    "        sampled_trX, sampled_trT = self.get_X_T(sampled_origins, sampled_dests)\n",
    "\n",
    "\n",
    "        return sampled_trX, sampled_trT, sampled_origins\n",
    "\n",
    "    def __getitem_tile__(self, index: int) -> Tuple[Any, Any]:\n",
    "        'Generates one sample of data (one tile)'\n",
    "\n",
    "        tileid2oa2features2vals = self.tileid2oa2features2vals\n",
    "        dim_dests = self.dim_dests\n",
    "        tile_ID = self.list_IDs[index]\n",
    "        sampled_origins = list(tileid2oa2features2vals[tile_ID].keys())\n",
    "\n",
    "        # Select a subset of OD pairs\n",
    "        train_locs = sampled_origins\n",
    "        all_locs_in_train_region = train_locs\n",
    "        size_train_dest = min(dim_dests, len(all_locs_in_train_region))\n",
    "        sampled_dests = [self.get_destinations(oa, size_train_dest, all_locs_in_train_region)\n",
    "                         for oa in sampled_origins]\n",
    "\n",
    "        # get the features and flows\n",
    "        sampled_trX, sampled_trT = self.get_X_T(sampled_origins, sampled_dests)\n",
    "\n",
    "        return sampled_trX, sampled_trT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee1c28-9687-4103-8af8-319ff56bebd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68cf5d82-c5b1-4371-bf7c-ce914a26df29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size N] [--test-batch-size N]\n",
      "                             [--epochs N] [--lr LR] [--momentum M] [--seed S]\n",
      "                             [--log-interval N] [--device DEVICE]\n",
      "                             [--mode MODE]\n",
      "                             [--tessellation-area TESSELLATION_AREA]\n",
      "                             [--tessellation-size TESSELLATION_SIZE]\n",
      "                             [--dataset DATASET]\n",
      "                             [--tile-id-column TILE_ID_COLUMN]\n",
      "                             [--tile-geometry TILE_GEOMETRY]\n",
      "                             [--oa-id-column OA_ID_COLUMN]\n",
      "                             [--oa-geometry OA_GEOMETRY]\n",
      "                             [--flow-origin-column FLOW_ORIGIN_COLUMN]\n",
      "                             [--flow-destination-column FLOW_DESTINATION_COLUMN]\n",
      "                             [--flow-flows-column FLOW_FLOWS_COLUMN]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jovyan/.local/share/jupyter/runtime/kernel-e127da28-1f25-410b-b9de-d83ca6d6c1b9.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from importlib.machinery import SourceFileLoader\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='DeepGravity')\n",
    "parser.add_argument('--batch_size', type=int, default=1, metavar='N',\n",
    "                    help='input batch size for training (default: 1)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1, metavar='N',\n",
    "                    help='input batch size for testing (default: 1)')\n",
    "parser.add_argument('--epochs', type=int, default=15, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=5e-6, metavar='LR',\n",
    "                    help='learning rate (default: 5e-6)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "parser.add_argument('--seed', type=int, default=1234, metavar='S',\n",
    "                    help='random seed (default: 1234)')\n",
    "parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--device', default='cpu',\n",
    "                    help='Wheter this is running on cpu or gpu')\n",
    "parser.add_argument('--mode', default='train', help='Can be train or test')\n",
    "# Model arguments\n",
    "parser.add_argument('--tessellation-area', default='United Kingdom',\n",
    "                    help='The area to tessel if a tessellation is not provided')\n",
    "parser.add_argument('--tessellation-size', type=int, default=25000,\n",
    "                    help='The tessellation size (meters) if a tessellation is not provided')\n",
    "parser.add_argument('--dataset', default='new_york', help='The dataset to use')\n",
    "\n",
    "# Dataset arguments \n",
    "parser.add_argument('--tile-id-column', default='tile_ID', help='Column name of tile\\'s identifier')\n",
    "parser.add_argument('--tile-geometry', default='geometry', help='Column name of tile\\'s geometry')\n",
    "\n",
    "parser.add_argument('--oa-id-column', default='oa_ID', help='Column name of oa\\'s identifier')\n",
    "parser.add_argument('--oa-geometry', default='geometry', help='Column name of oa\\'s geometry')\n",
    "\n",
    "parser.add_argument('--flow-origin-column', default='origin', help='Column name of flows\\' origin')\n",
    "parser.add_argument('--flow-destination-column', default='destination', help='Column name of flows\\' destination')\n",
    "parser.add_argument('--flow-flows-column', default='flow', help='Column name of flows\\' actual value')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# global settings\n",
    "model_type = 'DG'\n",
    "data_name = args.dataset\n",
    "\n",
    "# random seeds\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "# loading DataLoader and utilities\n",
    "path = './data_loader.py'\n",
    "dgd = SourceFileLoader('dg_data', path).load_module()\n",
    "path = './utils.py'\n",
    "utils = SourceFileLoader('utils', path).load_module()\n",
    "\n",
    "# set the device \n",
    "args.cuda = args.device.find(\"gpu\") != -1\n",
    "\n",
    "if args.device.find(\"gpu\") != -1:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "\n",
    "# check if raw data exists and otherwise stop the execution\n",
    "if not os.path.isdir('./data/' + data_name):\n",
    "    raise ValueError('There is no dataset named ' + data_name + ' in ./data/')\n",
    "\n",
    "db_dir = './data/' + data_name\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    training_acc = 0.0\n",
    "\n",
    "    for batch_idx, data_temp in enumerate(train_loader):\n",
    "        b_data = data_temp[0]\n",
    "        b_target = data_temp[1]\n",
    "        ids = data_temp[2]\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "        for data, target in zip(b_data, b_target):\n",
    "\n",
    "            if args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            output = model.forward(data)\n",
    "            loss += model.loss(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            if batch_idx * len(b_data) == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} \\tLoss: {:.6f}'.format(epoch, batch_idx * len(b_data), len(train_loader),\n",
    "                                                                     loss.item() / args.batch_size))\n",
    "\n",
    "    running_loss = running_loss / len(train_dataset)\n",
    "    training_acc = training_acc / len(train_dataset)\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.\n",
    "        test_accuracy = 0.\n",
    "        n_origins = 0\n",
    "        for batch_idx, data_temp in enumerate(test_loader):\n",
    "            b_data = data_temp[0]\n",
    "            b_target = data_temp[1]\n",
    "            ids = data_temp[2]\n",
    "            test_loss = 0.0\n",
    "\n",
    "            for data, target in zip(b_data, b_target):\n",
    "                if args.cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                output = model.forward(data)\n",
    "                test_loss += model.loss(output, target).item()\n",
    "\n",
    "                cpc = model.get_cpc(data, target)\n",
    "                test_accuracy += cpc\n",
    "                n_origins += 1\n",
    "\n",
    "            break\n",
    "\n",
    "        test_loss /= n_origins\n",
    "        test_accuracy /= n_origins\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    loc2cpc_numerator = {}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data_temp in test_loader:\n",
    "            b_data = data_temp[0]\n",
    "            b_target = data_temp[1]\n",
    "            ids = data_temp[2]\n",
    "            for id, data, target in zip(ids, b_data, b_target):\n",
    "                if args.cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                output = model.forward(data)\n",
    "                cpc = model.get_cpc(data, target, numerator_only=True)\n",
    "                loc2cpc_numerator[id[0]] = cpc\n",
    "    edf = pd.DataFrame.from_dict(loc2cpc_numerator, columns=['cpc_num'], orient='index').reset_index().rename(\n",
    "        columns={'index': 'locID'})\n",
    "    oa2tile = {oa: t for t, v in tileid2oa2features2vals.items() for oa in v.keys()}\n",
    "\n",
    "    def cpc_from_num(edf, oa2tile, o2d2flow):\n",
    "        print(edf.head())\n",
    "        edf['tile'] = edf['locID'].apply(lambda x: oa2tile[x])\n",
    "        edf['tot_flow'] = edf['locID'].apply(lambda x: sum(o2d2flow[x].values()) if x in o2d2flow else 1e-6)\n",
    "        cpc_df = pd.DataFrame(edf.groupby('tile').apply( \\\n",
    "            lambda x: x['cpc_num'].sum() / 2 / x['tot_flow'].sum()), \\\n",
    "            columns=['cpc']).reset_index()\n",
    "        return cpc_df\n",
    "\n",
    "    cpc_df = cpc_from_num(edf, oa2tile, o2d2flow)\n",
    "    print('Average CPC of test tiles: {cpc_df.cpc.mean():.4f}  stdev: {cpc_df.cpc.std():.4f}')\n",
    "\n",
    "    fname = './results/tile2cpc_{}_{}.csv'.format(model_type, args.dataset)\n",
    "\n",
    "    cpc_df.to_csv(fname, index=False)\n",
    "\n",
    "\n",
    "utils.tessellation_definition(db_dir, args.tessellation_area, args.tessellation_size)\n",
    "\n",
    "tileid2oa2features2vals, oa_gdf, flow_df, oa2pop, oa2features, od2flow, oa2centroid = utils.load_data(db_dir,\n",
    "                                                                                                      args.tile_id_column,\n",
    "                                                                                                      args.tile_geometry,\n",
    "                                                                                                      args.oa_id_column,\n",
    "                                                                                                      args.oa_geometry,\n",
    "                                                                                                      args.flow_origin_column,\n",
    "                                                                                                      args.flow_destination_column,\n",
    "                                                                                                      args.flow_flows_column)\n",
    "\n",
    "oa2features = {oa: [np.log(oa2pop[oa])] + feats for oa, feats in oa2features.items()}\n",
    "\n",
    "o2d2flow = {}\n",
    "for (o, d), f in od2flow.items():\n",
    "    try:\n",
    "        d2f = o2d2flow[o]\n",
    "        d2f[d] = f\n",
    "    except KeyError:\n",
    "        o2d2flow[o] = {d: f}\n",
    "\n",
    "train_dataset_args = {'tileid2oa2features2vals': tileid2oa2features2vals,\n",
    "                      'o2d2flow': o2d2flow,\n",
    "                      'oa2features': oa2features,\n",
    "                      'oa2pop': oa2pop,\n",
    "                      'oa2centroid': oa2centroid,\n",
    "                      'dim_dests': 512,\n",
    "                      'frac_true_dest': 0.0,\n",
    "                      'model': model_type}\n",
    "\n",
    "test_dataset_args = {'tileid2oa2features2vals': tileid2oa2features2vals,\n",
    "                     'o2d2flow': o2d2flow,\n",
    "                     'oa2features': oa2features,\n",
    "                     'oa2pop': oa2pop,\n",
    "                     'oa2centroid': oa2centroid,\n",
    "                     'dim_dests': int(1e9),\n",
    "                     'frac_true_dest': 0.0,\n",
    "                     'model': model_type}\n",
    "\n",
    "# datasets\n",
    "train_data = [oa for t in pd.read_csv(db_dir + '/processed/train_tiles.csv', header=None, dtype=object)[0].values for oa\n",
    "              in tileid2oa2features2vals[str(t)].keys()]\n",
    "test_data = [oa for t in pd.read_csv(db_dir + '/processed/test_tiles.csv', header=None)[0].values for oa in\n",
    "             tileid2oa2features2vals[str(t)].keys()]\n",
    "\n",
    "train_dataset = dgd.FlowDataset(train_data, **train_dataset_args)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "\n",
    "test_dataset = dgd.FlowDataset(test_data, **test_dataset_args)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size)\n",
    "\n",
    "dim_input = len(train_dataset.get_features(train_data[0], train_data[0]))\n",
    "\n",
    "if args.mode == 'train':\n",
    "\n",
    "    model = utils.instantiate_model(oa2centroid, oa2features, oa2pop, dim_input, device=torch_device)\n",
    "    if args.device.find(\"gpu\") != -1:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    t0 = time.time()\n",
    "    test()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        # set new random seeds\n",
    "        torch.manual_seed(args.seed + epoch)\n",
    "        np.random.seed(args.seed + epoch)\n",
    "        random.seed(args.seed + epoch)\n",
    "\n",
    "        train(epoch)\n",
    "        test()\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: %s seconds\" % (t1 - t0))\n",
    "\n",
    "    fname = './results/model_{}_{}.pt'.format(model_type, args.dataset)\n",
    "    print('Saving model to {} ...'.format(fname))\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, fname)\n",
    "\n",
    "    print('Computing the CPC on test set, loc2cpc_numerator ...')\n",
    "\n",
    "    evaluate()\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    model = utils.instantiate_model(oa2centroid, oa2features, oa2pop, dim_input, device=torch_device)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    checkpoint = torch.load('./results/model_' + model_type + '_' + args.dataset + '.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1592a51-08bf-4023-bbf0-2b12d987d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# Create the parser with a different program name\n",
    "parser = argparse.ArgumentParser(prog='my_program')\n",
    "\n",
    "# Add the dataset arguments\n",
    "parser.add_argument('--tile-id-column', default='tile_ID', help='Column name of tile\\'s identifier')\n",
    "parser.add_argument('--tile-geometry', default='geometry', help='Column name of tile\\'s geometry')\n",
    "\n",
    "parser.add_argument('--oa-id-column', default='oa_ID', help='Column name of oa\\'s identifier')\n",
    "parser.add_argument('--oa-geometry', default='geometry', help='Column name of oa\\'s geometry')\n",
    "\n",
    "parser.add_argument('--flow-origin-column', default='origin', help='Column name of flows\\' origin')\n",
    "parser.add_argument('--flow-destination-column', default='destination', help='Column name of flows\\' destination')\n",
    "parser.add_argument('--flow-flows-column', default='flow', help='Column name of flows\\' actual value')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Access the values of the arguments\n",
    "tile_id_column = args.tile_id_column\n",
    "tile_geometry = args.tile_geometry\n",
    "oa_id_column = args.oa_id_column\n",
    "oa_geometry = args.oa_geometry\n",
    "flow_origin_column = args.flow_origin_column\n",
    "flow_destination_column = args.flow_destination_column\n",
    "flow_flows_column = args.flow_flows_column\n",
    "\n",
    "# Use the argument values in your code\n",
    "print(\"Tile ID Column:\", tile_id_column)\n",
    "print(\"Tile Geometry:\", tile_geometry)\n",
    "print(\"OA ID Column:\", oa_id_column)\n",
    "print(\"OA Geometry:\", oa_geometry)\n",
    "print(\"Flow Origin Column:\", flow_origin_column)\n",
    "print(\"Flow Destination Column:\", flow_destination_column)\n",
    "print(\"Flow Flows Column:\", flow_flows_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a475eac-408f-4427-a891-6f879f6aa172",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3789462702.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [10]\u001b[0;36m\u001b[0m\n\u001b[0;31m    main.py --batch_size 1 --test-batch-size 1 --epochs 15 --lr 5e-6 --momentum 0.9 --seed 1234 --log-interval 1 --device cpu --mode train --tessellation-area \"United Kingdom\" --tessellation-size 25000 --dataset new_york --tile-id-column tile_ID --tile-geometry geometry --oa-id-column oa_ID --oa-geometry geometry --flow-origin-column origin --flow-destination-column destination --flow-flows-column flow\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "main.py --batch_size 1 --test-batch-size 1 --epochs 15 --lr 5e-6 --momentum 0.9 --seed 1234 --log-interval 1 --device cpu --mode train --tessellation-area \"United Kingdom\" --tessellation-size 25000 --dataset new_york --tile-id-column tile_ID --tile-geometry geometry --oa-id-column oa_ID --oa-geometry geometry --flow-origin-column origin --flow-destination-column destination --flow-flows-column flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab8f55-d17d-43e4-90a1-6fb634c66a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
